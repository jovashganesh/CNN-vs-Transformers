{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "from tensorflow.keras.layers import Input \n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL.Image import fromarray\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve y_train values from csv\n",
    "# retrieve the x_train file names\n",
    "files_ids = []\n",
    "y_trainn =[]\n",
    "\n",
    "prefix = r\"C:/Users/FYP/Documents/eyedata/\"\n",
    "with open('{}trainLabels.csv'.format(prefix)) as file:\n",
    "    reader = list(csv.reader(file, delimiter=','))\n",
    "    for row in reader:\n",
    "        files_ids.append(r\"C:/Users/FYP/Documents/eyedata/400x400/train/\" + row[0] + \".jpeg\")\n",
    "        y_trainn.append(row[1])\n",
    "files_ids = files_ids[1:]\n",
    "y_trainn = [int(i) for i in y_trainn[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all x_train images and append into array\n",
    "x_train = []\n",
    "\n",
    "x_train_ids = []\n",
    "def get_x_train_shrinked():\n",
    "    for x in files_ids:\n",
    "        x_train_ids.append(r\"C:/Users/FYP/Documents/eyedata/400x400/train/\" + os.path.basename(x))\n",
    "        image = Image.open(x)\n",
    "        x_train.append(np.asarray(image, dtype=np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_x_train_shrinked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train into numpy array\n",
    "x_train = np.array(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the y_train values corresponding to the x_train file name\n",
    "y_train = []\n",
    "for i in x_train_ids:\n",
    "    for j in range(len(files_ids)):\n",
    "        if i == files_ids[j]:\n",
    "            y_train.append(y_trainn[j])\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 25808, 2: 5292, 1: 2443, 3: 873, 4: 708})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Class')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Counter(y_train))\n",
    "w = Counter(y_train)\n",
    "plt.bar(w.keys(), w.values())\n",
    "plt.title('Data')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_test split at 30% with stratify\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "oversample = SMOTE()\n",
    "\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy={0:5000},random_state=42)\n",
    "size = len(X_train)\n",
    "X_train = X_train.reshape(size, -1)\n",
    "X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "X_train = X_train.reshape(-1,300,300,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "manual_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record in training dataset: 25000\n",
      "Record in testing dataset: 7025\n",
      "Training dataset has 5000 records for the majority class and 20000 records for the minority class.\n"
     ]
    }
   ],
   "source": [
    "totalMaj = sorted(Counter(y_train).items())[0][1]\n",
    "totalMin = sorted(Counter(y_train).items())[1][1] + sorted(Counter(y_train).items())[2][1] + sorted(Counter(y_train).items())[3][1] + sorted(Counter(y_train).items())[4][1]\n",
    "\n",
    "print('Record in training dataset:', X_train.shape[0])\n",
    "print('Record in testing dataset:', X_val.shape[0])\n",
    "print(f\"Training dataset has {totalMaj} records for the majority class and {totalMin} records for the minority class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python image generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 5 classes.\n",
      "Found 7025 images belonging to 5 classes.\n",
      "Found 53576 images belonging to 1 classes.\n",
      "Found 7025 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Train generator for only training\n",
    "train_generator = validation_datagen.flow_from_directory(\n",
    "    directory=\"C:/Users/FYP/Documents/eyedata/dataflow/train/\",\n",
    "    target_size=(300,300),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "# Validation generator for validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory=\"C:/Users/FYP/Documents/eyedata/dataflow/validation/\",\n",
    "    target_size=(300,300),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42)\n",
    "# Prediction data for the x_validation data\n",
    "validation2_generator = validation_datagen.flow_from_directory(\n",
    "    directory=\"C:/Users/FYP/Documents/eyedata/dataflow/validation/\",\n",
    "    target_size=(300,300),\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the xception CNN model with 300x300 image size\n",
    "input_shape=(300, 300, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape)\n",
    "base_model = tf.keras.applications.Xception(include_top=False, input_tensor=img_input, input_shape=input_shape, pooling=\"max\", weights='imagenet')\n",
    "\n",
    "#train on all the layers of the pre-trained model\n",
    "base_model.trainable = True\n",
    "x = base_model.output\n",
    "predictions = Dense(5, activation=\"softmax\", name=\"predictions\")(x)\n",
    "model = Model(inputs=img_input, outputs=predictions)\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# save model at every iteration where the accuracy increases\n",
    "checkpoint = ModelCheckpoint(filepath='bestmodel.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit_generator(generator=train_generator, validation_data=validation_generator,\n",
    "          epochs=15,\n",
    "          callbacks=[checkpoint]\n",
    "        #   class_weight=manual_weights\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print graphs for the accuracy and loss of model\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate model on the validation set\n",
    "STEP_SIZE_TEST=validation2_generator.n//validation2_generator.batch_size\n",
    "validation2_generator.reset()\n",
    "pred=model.predict_generator(validation2_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 4868, 2: 1553, 1: 307, 4: 191, 3: 106})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted values of all images on the validation set\n",
    "predicted_x_val=np.argmax(pred,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the accuracy and the confusion matrix\n",
    "print(\"Accuracy on testing data: %2.2f%% \" % (accuracy_score(y_val,predicted_x_val)))\n",
    "print(classification_report(y_val,predicted_x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "confusion = sns.heatmap(confusion_matrix(y_val,predicted_x_val), annot=True, fmt='d', cbar=False, cmap=plt.cm.Blues)\n",
    "confusion.set_xlabel(\"Actual Label\", fontsize = 10)\n",
    "confusion.set_ylabel(\"Predicted Label\", fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive the cohen's kappa score\n",
    "cohen_kappa_score(predicted_x_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39a5cb20e6aabec12210803d2966054fd1879e71cc76d312edcbeba71d02f2af"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf2.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
